{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430dd73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, make_scorer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99301db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f07d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
    "rcParams['figure.figsize'] = 14, 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f4aa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "LABELS = [\"Normal\", \"Fraud\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffc354f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_path = 'C:/Users/hadad/PycharmProjects/Fraud_detection/data/creditcard.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "print(\"Data loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a67dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "features = data.columns.drop(['Class', 'Time'])\n",
    "X = data[features]\n",
    "y = data['Class']\n",
    "print(\"Data preprocessing completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae8a35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"Data standardized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af950587",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=RANDOM_SEED)\n",
    "print(\"Data split into train and test sets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c902e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=RANDOM_SEED),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=RANDOM_SEED),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=RANDOM_SEED, n_estimators=10, max_depth=5),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=RANDOM_SEED, n_estimators=10),\n",
    "    \"Support Vector Machine\": SVC(probability=True, random_state=RANDOM_SEED),\n",
    "    \"XGBoost\": XGBClassifier(eval_metric='logloss', random_state=RANDOM_SEED, n_estimators=10),\n",
    "    \"LightGBM\": LGBMClassifier(random_state=RANDOM_SEED, n_estimators=10)\n",
    "}\n",
    "print(\"Models defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3ad387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define scoring metrics\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'f1': make_scorer(f1_score),\n",
    "    'roc_auc': make_scorer(roc_auc_score)\n",
    "}\n",
    "print(\"Scoring metrics defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe5810b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate models\n",
    "results = {}\n",
    "for model_name, model in models.items():\n",
    "    start_time = time.time()\n",
    "    print(f\"Evaluating {model_name}...\")\n",
    "    grid = GridSearchCV(estimator=model, param_grid={}, scoring=scoring, refit='roc_auc', cv=2)\n",
    "    grid_result = grid.fit(X_train, y_train)\n",
    "    results[model_name] = grid_result\n",
    "    end_time = time.time()\n",
    "    print(f\"{model_name} evaluated in {end_time - start_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26c7a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize results\n",
    "summary = []\n",
    "for model_name, grid_result in results.items():\n",
    "    summary.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Best Score (ROC AUC)\": grid_result.best_score_,\n",
    "        \"Best Params\": grid_result.best_params_\n",
    "    })\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Best Score (ROC AUC): {grid_result.best_score_}\")\n",
    "    print(f\"Best Params: {grid_result.best_params_}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b76580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for summary\n",
    "summary_df = pd.DataFrame(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a21b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the summary DataFrame\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6e2602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC AUC scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=\"Model\", y=\"Best Score (ROC AUC)\", hue=\"Model\", data=summary_df, palette=\"muted\", dodge=False)\n",
    "plt.title(\"Comparison of ROC AUC Scores for Different Models\")\n",
    "plt.ylabel(\"Best Score (ROC AUC)\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.legend([], [], frameon=False)  # حذف کردن legend\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20818e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Best Params as a heatmap\n",
    "summary_df['Best Score (ROC AUC)'] = summary_df['Best Score (ROC AUC)'].astype(float)\n",
    "pivot_table = summary_df.pivot_table(values=\"Best Score (ROC AUC)\", index=\"Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558bd46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(pivot_table, annot=True, cmap=\"viridis\", fmt=\".4f\")\n",
    "plt.title(\"Best ROC AUC Scores for Different Models\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac69928",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Script finished.\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
